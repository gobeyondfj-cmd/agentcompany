"""Base classes and data structures for the LLM provider abstraction layer."""

from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import AsyncIterator


@dataclass
class LLMMessage:
    """A single message in a conversation with an LLM.

    Parameters
    ----------
    role:
        One of ``"system"``, ``"user"``, ``"assistant"``, or ``"tool"``.
    content:
        The textual content of the message.
    tool_calls:
        Tool invocations returned by the assistant (present only when
        ``role == "assistant"``).
    tool_call_id:
        The identifier linking a tool result back to its originating call
        (present only when ``role == "tool"``).
    """

    role: str  # "system", "user", "assistant", "tool"
    content: str
    tool_calls: list[dict] | None = None
    tool_call_id: str | None = None


@dataclass
class ToolDefinition:
    """Schema describing a tool the LLM may invoke.

    Parameters
    ----------
    name:
        A unique, short identifier for the tool.
    description:
        Human-readable explanation of what the tool does.
    parameters:
        A JSON Schema dictionary describing the tool's accepted arguments.
    """

    name: str
    description: str
    parameters: dict  # JSON Schema


@dataclass
class ToolCall:
    """A parsed tool invocation returned by the LLM.

    Parameters
    ----------
    id:
        Provider-assigned identifier used to pair this call with its result.
    name:
        The name of the tool being invoked.
    arguments:
        The parsed argument dictionary for the tool.
    """

    id: str
    name: str
    arguments: dict


@dataclass
class LLMResponse:
    """The result of a non-streaming LLM completion.

    Parameters
    ----------
    content:
        The textual content generated by the model (may be empty when only
        tool calls are returned).
    tool_calls:
        Any tool invocations the model chose to make.
    usage:
        Token usage information, e.g.
        ``{"input_tokens": 100, "output_tokens": 50}``.
    stop_reason:
        The reason the model stopped generating (provider-specific string).
    """

    content: str
    tool_calls: list[ToolCall] | None = None
    usage: dict | None = None
    stop_reason: str | None = None


class BaseLLMProvider(ABC):
    """Abstract base class that all LLM provider implementations must extend.

    Parameters
    ----------
    api_key:
        The authentication key for the provider's API.
    model:
        The model identifier to use for completions.
    base_url:
        Optional override for the API endpoint (useful for proxies or
        self-hosted OpenAI-compatible servers).
    max_tokens:
        The maximum number of tokens the model may generate per request.
    """

    def __init__(
        self,
        api_key: str,
        model: str,
        base_url: str | None = None,
        max_tokens: int = 4096,
    ):
        if not api_key:
            raise ValueError(
                f"{self.__class__.__name__} requires a non-empty api_key. "
                "Please set the appropriate API key in your configuration or "
                "environment variables."
            )
        if not model:
            raise ValueError(
                f"{self.__class__.__name__} requires a non-empty model name."
            )

        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.max_tokens = max_tokens

    @abstractmethod
    async def complete(
        self,
        messages: list[LLMMessage],
        tools: list[ToolDefinition] | None = None,
    ) -> LLMResponse:
        """Send a completion request and return the full response.

        Parameters
        ----------
        messages:
            The conversation history.
        tools:
            Optional list of tools the model is allowed to invoke.

        Returns
        -------
        LLMResponse
            The model's response including any tool calls.
        """
        ...

    @abstractmethod
    async def stream(
        self,
        messages: list[LLMMessage],
        tools: list[ToolDefinition] | None = None,
    ) -> AsyncIterator[str]:
        """Stream a completion, yielding text deltas as they arrive.

        Parameters
        ----------
        messages:
            The conversation history.
        tools:
            Optional list of tools the model is allowed to invoke.

        Yields
        ------
        str
            Incremental text chunks produced by the model.
        """
        ...
